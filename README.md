# ClinicalNLP2024

Python code for computing LLMs surprisals and linear machine learning models

Paper: Leveraging pre-trained large language models for aphasia detection in English and Chinese speakers

Authors: Yan Cong, Jiyeon Lee, Arianna N. LaCroix

@inproceedings{cong-etal-2024-leveraging,
    title = "Leveraging pre-trained large language models for aphasia detection in {E}nglish and {C}hinese speakers",
    author = "Cong, Yan  and
      Lee, Jiyeon  and
      LaCroix, Arianna",
    editor = "Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Bitterman, Danielle",
    booktitle = "Proceedings of the 6th Clinical Natural Language Processing Workshop",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.clinicalnlp-1.20",
    pages = "238--245",
    abstract = "We explore the utility of pre-trained Large Language Models (LLMs) in detecting the presence, subtypes, and severity of aphasia across English and Mandarin Chinese speakers. Our investigation suggests that even without fine-tuning or domain-specific training, pre-trained LLMs can offer some insights on language disorders, regardless of speakers{'} first language. Our analysis also reveals noticeable differences between English and Chinese LLMs. While the English LLMs exhibit near-chance level accuracy in subtyping aphasia, the Chinese counterparts demonstrate less than satisfactory performance in distinguishing between individuals with and without aphasia. This research advocates for the importance of linguistically tailored and specified approaches in leveraging LLMs for clinical applications, especially in the context of multilingual populations.",
}


